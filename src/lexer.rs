use serde::Serialize;
use smol_str::SmolStr;

// Choose between std and alloc
cfg_if::cfg_if! {
    if #[cfg(feature = "std")] {
        extern crate std;
        use std::prelude::v1::*;
        use std::fmt;
    } else {
        extern crate alloc;
        use alloc::string::*;
        use alloc::vec::*;
        use alloc::vec;
        use alloc::format;
        use core::fmt;
    }
}

#[derive(Serialize, Debug, PartialEq, Eq, Clone, Hash)]
pub enum TokenKinds {
    /// A sequence of characters
    Token(SmolStr),
    /// A string of characters that will be generated by the preprocessor
    Complex(SmolStr),
    Text,
    Whitespace,
    Control(ControlTokenKind),
}

impl TokenKinds {
    pub fn is_whitespace(&self) -> bool {
        match self {
            TokenKinds::Whitespace => true,
            TokenKinds::Control(ControlTokenKind::Eol) => true,
            _ => false,
        }
    }
}

#[derive(Debug, PartialEq, Eq, Clone, Hash, Serialize)]
pub enum ControlTokenKind {
    Eof,
    Eol,
}

pub type Preprocessor = fn(text: &str, tokens: Vec<Token>) -> Result<Vec<Token>, PreprocessorError>;

pub struct PreprocessorError {
    pub message: String,
    pub location: TextLocation,
    pub len: usize,
}

impl fmt::Debug for PreprocessorError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "An error occurred during lexing at line {} column {}: {}",
            self.location.line, self.location.column, self.message
        )
    }
}

impl fmt::Display for PreprocessorError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "An error occurred during lexing at line {} column {}: {}",
            self.location.line, self.location.column, self.message
        )
    }
}

#[derive(Serialize, Debug, Clone)]
pub struct Lexer {
    /// Possible token kinds
    pub(crate) token_kinds: Vec<SmolStr>,
    longest_token_size: usize,
    #[serde(skip, default)]
    pub preprocessors: Vec<Preprocessor>,
}

#[derive(Debug, PartialEq, Eq, Clone, Hash, Serialize)]
pub struct Token {
    /// Index of the token in the text
    pub index: usize,
    /// Length of the token
    pub len: usize,
    /// Location for debugging
    pub location: TextLocation,
    /// Kind of token
    pub kind: TokenKinds,
}

#[derive(Debug, PartialEq, Eq, Clone, Copy, Hash, Serialize)]
pub struct TextLocation {
    pub line: usize,
    pub column: usize,
    pub index: usize,
    pub len: usize,
}

impl TextLocation {
    pub fn new(line: usize, column: usize, index: usize, len: usize) -> TextLocation {
        let line = line + 1;
        let column = column + 1;
        TextLocation {
            line,
            column,
            index,
            len,
        }
    }
}

impl Token {
    pub fn stringify<'a>(&self, txt: &'a str) -> &'a str {
        &txt[self.index..self.index + self.len]
    }

    pub fn stringify_until<'a>(&self, other: &Self, txt: &'a str) -> &'a str {
        &txt[self.index..other.index + other.len]
    }
}

impl fmt::Display for TokenKinds {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            TokenKinds::Token(smol_str) => write!(f, "{smol_str}"),
            TokenKinds::Complex(smol_str) => write!(f, "{smol_str}"),
            TokenKinds::Text => write!(f, "<text>"),
            TokenKinds::Whitespace => write!(f, "<whitespace>"),
            TokenKinds::Control(ctk) => write!(f, "{ctk}"),
        }
    }
}

impl fmt::Display for ControlTokenKind {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            ControlTokenKind::Eof => write!(f, "End of file"),
            ControlTokenKind::Eol => write!(f, "New line"),
        }
    }
}

impl Default for Lexer {
    fn default() -> Self {
        Self::new()
    }
}

impl Lexer {
    pub fn new() -> Lexer {
        Lexer {
            token_kinds: Vec::new(),
            longest_token_size: 0,
            preprocessors: Vec::new(),
        }
    }

    pub fn add_tokens<T>(&mut self, tokens: impl Iterator<Item = T>)
    where
        T: Into<SmolStr>,
    {
        for into_token in tokens {
            let token = into_token.into();
            if token.len() > self.longest_token_size {
                self.longest_token_size = token.len();
            }
            self.add_token(token.clone());
        }
    }

    pub fn add_token(&mut self, token: impl Into<SmolStr>) {
        let token = token.into();
        if token.len() > self.longest_token_size {
            self.longest_token_size = token.len();
        }
        // find the right place to insert the token
        //
        //  1. find the first token that is longer than the new token
        //  2. insert the new token before the first token that is longer
        //
        // This way the tokens are sorted by length
        // This is important for optimization of the lexer
        let index = self
            .token_kinds
            .iter()
            .position(|x| x.len() > token.len())
            .unwrap_or(self.token_kinds.len());
        self.token_kinds.insert(index, token);
    }

    /// Lexer for UTF-8 text
    pub fn lex_utf8(&self, text: &str) -> Result<Vec<Token>, PreprocessorError> {
        let chars = text.char_indices().collect::<Vec<(usize, char)>>();
        let len = chars.len();
        // the allocation is a guess, but it should be close enough
        let mut tokens = Vec::with_capacity(chars.len() / 4);
        let mut i = 0;
        let mut line = 0;
        let mut column = 0;
        'chars: while i < len {
            // Take new line into account
            if chars[i].1 == '\n' {
                line += 1;
                column = 0;
                tokens.push(Token {
                    index: chars[i].0,
                    len: 1,
                    location: TextLocation::new(line, column, chars[i].0, 1),
                    kind: TokenKinds::Control(ControlTokenKind::Eol),
                });
                i += 1;
                continue;
            }

            // Match token kinds
            'tokens: for token_kind in self.token_kinds.iter().rev() {
                let tok_chars = token_kind.char_indices();
                let tok_len = tok_chars.count();
                if i + tok_len > len {
                    // All the remaining tokens are longer than the remaining text
                    //
                    // This is a performance optimization
                    continue;
                }
                for (j, (_, c)) in token_kind.char_indices().enumerate() {
                    if c != chars[i + j].1 {
                        continue 'tokens;
                    }
                }
                tokens.push(Token {
                    index: chars[i].0,
                    len: token_kind.len(),
                    location: TextLocation::new(line, column, chars[i].0, token_kind.len()),
                    kind: TokenKinds::Token(token_kind.clone()),
                });
                i += tok_len;
                column += tok_len;
                continue 'chars;
            }

            // Match whitespace
            if chars[i].1.is_whitespace() {
                tokens.push(Token {
                    index: chars[i].0,
                    len: 1,
                    location: TextLocation::new(line, column, chars[i].0, 1),
                    kind: TokenKinds::Whitespace,
                });
                i += 1;
                column += 1;
                continue;
            }

            // Match text until next whitespace/token/eof
            let mut j = 0;
            let mut token_len = 0;
            'word: while i + j < len {
                if chars[i + j].1.is_whitespace() {
                    break;
                }
                token_len += chars[i + j].1.len_utf8();
                j += 1;
                for token_kind in &self.token_kinds {
                    let start = i + j;
                    let tok_len = token_kind.chars().count();
                    let end = if i + j + tok_len < len {
                        i + j + tok_len
                    } else {
                        break 'word;
                    };
                    let token = &text[chars[start].0..chars[end].0];
                    if token == *token_kind {
                        break 'word;
                    }
                }
            }
            tokens.push(Token {
                index: chars[i].0,
                len: token_len,
                location: TextLocation::new(line, column, chars[i].0, token_len),
                kind: TokenKinds::Text,
            });
            column += j;
            i += j;
        }
        tokens.push(Token {
            index: i,
            len: 0,
            location: TextLocation::new(
                line,
                column,
                chars.last().map(|(i, _)| *i).unwrap_or(0),
                0,
            ),
            kind: TokenKinds::Control(ControlTokenKind::Eof),
        });

        for preprocessor in &self.preprocessors {
            tokens = preprocessor(text, tokens)?;
        }

        Ok(tokens)
    }

    /// Lexer for ascii-only text
    pub fn lex_ascii(&self, text: &str) -> Result<Vec<Token>, PreprocessorError> {
        let chars = text.as_bytes();
        // the allocation is a guess, but it should be close enough
        let mut tokens = Vec::with_capacity(chars.len() / 4);
        let mut i = 0;
        let mut line = 0;
        let mut column = 0;
        let len = chars.len();
        'chars: while i < len {
            // Take new line into account
            if chars[i] == b'\n' {
                line += 1;
                column = 0;
                i += 1;
                tokens.push(Token {
                    index: i,
                    len: 1,
                    location: TextLocation::new(line, column, i, 1),
                    kind: TokenKinds::Control(ControlTokenKind::Eol),
                });
                continue;
            }

            for token_kind in self.token_kinds.iter().rev() {
                let tok_len = token_kind.len();
                if i + tok_len > len {
                    // All the remaining tokens are longer than the remaining text
                    //
                    // This is a performance optimization
                    continue;
                }
                let token = &chars[i..i + tok_len];
                if token == token_kind.as_bytes() {
                    tokens.push(Token {
                        index: i,
                        len: tok_len,
                        location: TextLocation::new(line, column, i, tok_len),
                        kind: TokenKinds::Token(token_kind.clone()),
                    });
                    i += tok_len;
                    column += tok_len;
                    continue 'chars;
                }
            }

            // Match whitespace
            if (chars[i] as char).is_whitespace() {
                tokens.push(Token {
                    index: i,
                    len: 1,
                    location: TextLocation::new(line, column, i, 1),
                    kind: TokenKinds::Whitespace,
                });
                i += 1;
                column += 1;
                continue;
            }

            // Match text until next whitespace/token/eof
            let mut j = 0;
            'word: while i + j < len {
                if (chars[i + j] as char).is_whitespace() {
                    break;
                }
                j += 1;
                for token_kind in &self.token_kinds {
                    let start = i + j;
                    let tok_len = token_kind.len();
                    let end = if i + j + tok_len <= len {
                        i + j + tok_len
                    } else {
                        break 'word;
                    };
                    let token = &text[start..end];
                    if token == *token_kind {
                        break 'word;
                    }
                }
            }
            tokens.push(Token {
                index: i,
                len: j,
                location: TextLocation::new(line, column, i, j),
                kind: TokenKinds::Text,
            });
            column += j;
            i += j;
        }

        tokens.push(Token {
            index: i,
            len: 0,
            location: TextLocation::new(line, column, i, 0),
            kind: TokenKinds::Control(ControlTokenKind::Eof),
        });

        for preprocessor in &self.preprocessors {
            tokens = preprocessor(text, tokens)?;
        }

        Ok(tokens)
    }
}
